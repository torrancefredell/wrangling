{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrancefredell/wrangling/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DS3001/wrangling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKAb5RHO3eBM",
        "outputId": "0b440346-f9bc-452a-d381-be0de1dd9d35"
      },
      "id": "LKAb5RHO3eBM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wrangling'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 63 (delta 22), reused 27 (delta 10), pack-reused 21\u001b[K\n",
            "Receiving objects: 100% (63/63), 6.75 MiB | 17.77 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Responses**\n",
        "1. This paper is about a framework that can be used to \"tidy\" datasets by developing tools and streamlining processes for this purpose, so that datasets can be easily analyzed and visualized.\n",
        "2. The tidy data standard was made to simplify data exploration and analysis, as well as to condense the number of tools used in order to spend more time drawing valuable insights from data, rather than trying to figure out how to fix it up.\n",
        "3. The first quote, which compares happy vs. messy datasets and families, aims to share the message that all happy families and datasets are happy in one way; they do not have major issues, and as a result they are all similar, with a common structure. However, just as a family could appear messy because of any number of factors, datasets are the same; they could be messy because of too many missing pieces, faulty column/row naming methods, incorrect data types, etc. The second sentence refers to the complexities of understanding what is a variable versus observation in a dataset. This is difficult because variables and observations are very much defined in the context of the study that they are a part of, so it is difficult to say that they will universally \"look\" a certain way or describe certain things.\n",
        "4. Values are data points, of which the dataset is mainly comprised. Variables are the various attributes of whatever is being studied, and observations are the different units that are measured for each variable.\n",
        "5. Tidy data is defined as data that has three requirements: every variable and observation is its own column and row, respectively, and all of the observational units form the table itself.\n",
        "6. The five most common problems with messy datasets are as follows: column headers are values when they should be variable names, columns contain multiple variables, variables can be found in both rows and columns, multiple types of observational units are in one table, and/or an observational unit can be found in multiple tables. The data in table four are messy because the column headers are values when they should be variable names. The variables are religion, income, and frequency, but the column headers (income ranges) should correspond with values in one column titled \"income\" inside the table. Melting a dataset refers to the process of taking several columns and combining them into a single column with some 'variable' header, as well as an associated 'value' column, which contains the original columns' data. Each original column header is thus a value in the new variable column.  \n",
        "7. Dataset 11 is messy for a few reasons: first, it has an excessive number of blank spots, marked by \"--.\" Second, each day of the month has its own column, when they could instead be condensed down to a single \"dates\" column with an associated \"values\" column next to it. Dataset 12 is tidy and molten because the 30+ columns dedicated to the date have been condensed to one column, which has an associated value column.\n",
        "8. The \"chicken and egg\" problem refers to the problem where tidy data and tidy tools are interdependent; either one could come first, but realistically, neither one could be properly developed without the other. Therefore, they must be simultaneously adopted and advanced. In the future, Wickham hopes that scientists will build off of and improve the tidy data framework. Additionally, he hopes that tools that work with values stored in multidimensional arrays will be created, as well as tools that can choose between array-tidy and dataframe-tidy formats, for memory and performance sakes. Along with tidying, other tools that he mentions related to data cleaning include ones that could potentially find and missing values or typos, parse dates, work with international data, etc."
      ],
      "metadata": {
        "id": "mYilmEBxteHZ"
      },
      "id": "mYilmEBxteHZ"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Responses**\n",
        "1. According to the US Census Bureau, race data is collected via self-identification, and individuals may choose to report more than one race group. They could check a box for things like \"White\" or \"African Am\" and then specify origin (\"Irish\" or \"Jamaican,\" for example) through print on the paper 2020 census.\n",
        "2. Race data are collected in order to aid governments and communities\n",
        "in their fight against discrimination. When used in conjunction with data related to housing, education, employment, etc., race data become a powerful tool to uncover inequalities that might otherwise be difficult to detect. These data collectively also allow for more just policy-making, since they help politicians understand demographic trends and the distribution of various resources. Good quality data is needed so that this entire process is accurate and so that policies can reflect the needs of the populations that they affect.\n",
        "3."
      ],
      "metadata": {
        "id": "yeJ275hM8FS7"
      },
      "id": "yeJ275hM8FS7"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
